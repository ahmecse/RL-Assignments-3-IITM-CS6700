# CS6700 Programming Assignment 3: Hierarchical Reinforcement Learning (Taxi Domain)

[![Python](https://img.shields.io/badge/python-3.x-blue.svg)](requirements.txt) <!-- Update Python version if known -->
<!-- [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE) --> <!-- Optional: Add license badge if you add a LICENSE file -->

An implementation and comparison of **SMDP Q-Learning** and **Intra-Option Q-Learning** algorithms for solving the **Taxi Domain** problem using **OpenAI Gymnasium**.

---

## ğŸ§ª Objectives

1.  **Implement SMDP Q-Learning:** Develop and apply the single-step Semi-Markov Decision Process (SMDP) Q-Learning algorithm to the Taxi Domain.
2.  **Implement Intra-Option Q-Learning:** Develop and apply Intra-Option Q-Learning using a defined set of options within the Taxi Domain.
3.  **Analyze & Compare:**
    *   Plot reward curves for both algorithms to visualize learning progress.
    *   Visualize the learned Q-values for key state-action pairs or options.
    *   Provide written descriptions of the learned policies or option behaviors.
4.  **Experiment:** Implement and evaluate an alternate set of options for Intra-Option Q-Learning and compare its performance against the initial set.

---

## ğŸ“‹ Table of Contents

1.  [Overview](#ğŸ“-overview)
2.  [Environment: Taxi Domain](#ğŸš•-environment-taxi-domain)
3.  [Algorithms Implemented](#âš™ï¸-algorithms-implemented)
4.  [Project Structure](#ğŸ—‚ï¸-project-structure)
5.  [Setup & Installation](#ğŸš€-setup--installation)
6.  [Usage](#â–¶ï¸-usage)
7.  [Results](#ğŸ“Š-results)
8.  [Contributing](#ğŸ¤-contributing)
9.  [License](#ğŸ“œ-license)

---

## ğŸ“ Overview

This project explores Hierarchical Reinforcement Learning (HRL) techniques by implementing and comparing two key algorithms:

*   **SMDP Q-Learning:** Learns values for actions over extended periods defined by options.
*   **Intra-Option Q-Learning:** Learns policies *within* options simultaneously with learning values over options.

The goal is to understand their learning dynamics, performance differences, and the impact of option design in the classic Taxi Domain environment.

---

## ğŸš• Environment: Taxi Domain

*   **Grid Size:** 5x5
*   **Task:** Pick up a passenger from one of four designated locations (R, G, B, Y) and drop them off at another.
*   **States:** 500 discrete states representing taxi location (row, col), passenger location (in taxi or at R, G, B, Y), and destination (R, G, B, Y).
*   **Actions:** 6 discrete actions (Move North, South, East, West, Pickup, Dropoff).
*   **Rewards:**
    *   `-1` per step.
    *   `+20` for successful drop-off.
    *   `-10` for illegal pickup/dropoff.
*   **Discount Factor (Î³):** 0.9
*   **Reference:** [Gymnasium Taxi-v3 Documentation](https://gymnasium.farama.org/environments/toy_text/taxi/)

---

## âš™ï¸ Algorithms Implemented

### 1. SMDP Q-Learning

This algorithm learns the value function Q(s, o) for initiating an option *o* in state *s*. It uses the Bellman equation adapted for options, considering the cumulative discounted reward and duration of executing an option.

### 2. Intra-Option Q-Learning

This approach learns Q(s, a) for primitive actions *a* within the context of an active option *o*. It allows for learning and improving the internal policies of options while the agent interacts with the environment. It typically involves learning both the option values Q(s, o) and the intra-option values Q(s, a).

*(Add more specific details about your implementation choices, option definitions, etc., here or link to the relevant code/notebook sections)*

---

## ğŸ—‚ï¸ Project Structure

```
CS6700-PA3-HRL-Taxi/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ hrl_taxi_domain_analysis.ipynb  # Main notebook for implementation, experiments & visualization
â”œâ”€â”€ src/                                # Source code (optional, if refactored)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ smdp_q_learning.py
â”‚   â”‚   â””â”€â”€ intra_option_q_learning.py
â”‚   â”œâ”€â”€ envs/
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ options/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ taxi_options.py             # Option definitions
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ plots/                          # Generated plots
â”‚   â”‚   â”œâ”€â”€ smdp_q_learning_rewards.png
â”‚   â”‚   â”œâ”€â”€ intra_option_q_learning_rewards.png
â”‚   â”‚   â””â”€â”€ comparison_plot.png
â”‚   â””â”€â”€ q_values/                       # Saved Q-value visualizations
â”‚       â”œâ”€â”€ smdp_q_values.png
â”‚       â””â”€â”€ intra_option_q_values.png
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ CS6700_PA3.pdf                  # Original assignment description
â”‚   â””â”€â”€ report.pdf                      # Project report PDF (if applicable)
â””â”€â”€ images/                             # Static images (if any)
```

---

## ğŸš€ Setup & Installation

1.  **Clone the repository:**
    ```bash
    # Replace with your actual repository URL
    git clone https://github.com/your-username/CS6700-PA3-HRL-Taxi.git
    cd CS6700-PA3-HRL-Taxi
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    # Ensure requirements.txt includes: gymnasium, numpy, matplotlib, etc.
    ```

---

## â–¶ï¸ Usage

All implementation, experiments, and visualizations are primarily conducted within the Jupyter notebook:

1.  Navigate to the `notebooks` directory:
    ```bash
    cd notebooks
    ```
2.  Launch Jupyter Lab or Jupyter Notebook:
    ```bash
    jupyter lab
    # or
    # jupyter notebook
    ```
3.  Open `hrl_taxi_domain_analysis.ipynb` and execute the cells sequentially.

*(Optional: If you create separate training scripts in `src/`, provide command-line usage examples here.)*

---

## ğŸ“Š Results

Performance comparison plots (e.g., rewards per episode) and Q-value visualizations are generated by the notebook and saved in the `results/` directory.

*(Embed key result plots here for quick viewing)*

**Reward Curves Comparison**

![Reward Plot](results/plots/comparison_plot.png) <!-- Replace with your actual comparison plot -->

**Summary of Findings:**

*(Add 1-2 sentences summarizing the key observations from the results, e.g., comparing convergence speed, final performance, and the impact of different option sets between SMDP Q-Learning and Intra-Option Q-Learning.)*

*(Refer to the `docs/report.pdf` or the analysis notebook for detailed results and discussion.)*

---

## ğŸ¤ Contributing

This repository is intended for an academic assignment. Contributions are generally not expected. However, feel free to open an issue if you find bugs or have suggestions.

---

## ğŸ“œ License

*(Optional: Specify the license. Create a LICENSE file if needed.)*

<!-- This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. -->

