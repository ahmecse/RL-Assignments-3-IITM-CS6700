# CS6700 Programming Assignment 3: Hierarchical Reinforcement Learning (Taxi Domain)

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](requirements.txt)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-0.29.1-brightgreen.svg)](https://pypi.org/project/gymnasium/)
[![NumPy](https://img.shields.io/badge/numpy-1.24+-orange.svg)](https://pypi.org/project/numpy/)
[![Matplotlib](https://img.shields.io/badge/matplotlib-3.5+-blueviolet.svg)](https://pypi.org/project/matplotlib/)
[![Jupyter](https://img.shields.io/badge/jupyter-notebook-yellow.svg)](https://jupyter.org/)
[![PyTorch](https://img.shields.io/badge/pytorch-1.13+-red.svg)](https://pytorch.org/)
[![tqdm](https://img.shields.io/badge/tqdm-progress--bar-lightgrey.svg)](https://pypi.org/project/tqdm/)


An implementation and comparison of **SMDP Q-Learning** and **Intra-Option Q-Learning** for solving the **Taxi Domain** problem using **OpenAI Gymnasium**.
---

## ğŸ§ª Objectives

1. **Implement SMDP Q-Learning:** Single-step Semi-Markov Q-Learning with pre-defined options.
2. **Implement Intra-Option Q-Learning:** Simultaneous learning of option values and intra-option policies.
3. **Analyze & Compare:**

   * Reward curves per episode
   * Learned Q-value heatmaps
   * Qualitative descriptions of option behaviors
4. **Experiment:** Evaluate an alternate, mutually exclusive option set and compare performance.

---

## ğŸ“‹ Table of Contents

1. [Overview](#ğŸ“-overview)
2. [Environment: Taxi Domain](#ğŸš•-environment-taxi-domain)
3. [Algorithms & Options](#âš™ï¸-algorithms--options)
4. [Hyperparameters](#ğŸ›ï¸-hyperparameters)
5. [Project Structure](#ğŸ—‚ï¸-project-structure)
6. [Setup & Installation](#ğŸš€-setup--installation)
7. [Usage](#â–¶ï¸-usage)
8. [Results](#ğŸ“Š-results)
9. [References](#ğŸ”–-references)


---

## ğŸ“ Overview

This assignment implements two hierarchical RL algorithms to solve the Taxiâ€‘v3 environment:

* **SMDP Q-Learning:** Learns optionâ€‘value function $Q(s, o)$ with termination and initiation sets.
* **Intra-Option Q-Learning:** Learns both $Q(s, o)$ and intra-option $Q(s, a)$ concurrently, enabling improvement of option policies during training.

We compare convergence speed, sample efficiency, and the impact of option design on final performance.

---

## ğŸš• Environment: Taxi Domain

* **Grid:** 5Ã—5, four pickup/dropoff locations labeled R, G, B, Y.
* **States (500):** Taxi row, column; passenger location (four sites or in-taxi); destination.
* **Actions (6):** North, South, East, West, Pickup, Dropoff.
* **Rewards:** Step âˆ’1; successful dropâ€‘off +20; illegal pickup/dropoff âˆ’10.
* **Discount factor (Î³):** 0.9
* **Gymnasium reference:** [Taxi-v3 docs](https://gymnasium.farama.org/environments/toy_text/taxi/)

### ğŸ–¼ï¸ Environment Map

![Taxi map](https://github.com/ahmecse/RL-Assignments-3-IITM-CS6700/raw/main/images/Visualizing%20Taxi-v3%20environment%20with%20positions%20annotated.png)

---

## âš™ï¸ Algorithms & Options

### Option Definitions (Initial Set)

* **Go-To-R/G/B/Y:** Moves taxi (via shortest path) to one of the four stations; terminates on arrival.
* **Primitive Options:** Pickup and Dropoff executed only if valid; terminate immediately.

### Alternate Option Set (Experiment)

* **Row-Complete:** Drive to correct row of target location, then terminate.
* **Column-Complete:** Drive to correct column of target location, then terminate.
* **Pickup/Dropoff:** As above.

### Algorithmic Details

* **SMDP Q-Learning:** Bellman update over options with duration $k$ and cumulative reward $R$.
* **Intra-Option Q-Learning:** Off-policy updates to both $Q(s,o)$ and intra-option $Q(s,a)$ per Sutton & Precup (1999).

---

## ğŸ›ï¸ Hyperparameters

| Parameter         | Value              | Notes               |
| ----------------- | ------------------ | ------------------- |
| Learning rate (Î±) | 0.1                | Constant            |
| Discount (Î³)      | 0.9                | â€”                   |
| Îµâ€‘greedy start    | 1.0 â†’ 0.1 over 50k | Linear decay        |
| Episodes          | 100,000            | Converges by \~80k  |
| Batch updates     | Online             | Single-step updates |

---

## ğŸ—‚ï¸ Project Structure

```
CS6700-PA3-HRL-Taxi/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ CS6700_PA3.pdf          # Assignment spec
â”‚   â””â”€â”€ report.pdf              # Detailed report
â”œâ”€â”€ images/                     # Static visuals
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ hrl_taxi_domain_analysis.ipynb  # Code & experiments
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ comparison_plots/
â”‚   â”‚   â”œâ”€â”€ smdp_q_learning_rewards.png
â”‚   â”‚   â”œâ”€â”€ intra_option_q_learning_rewards.png
â”‚   â”‚   â””â”€â”€ comparison_plot.png
â”‚   â””â”€â”€ q_values/
â”‚       â”œâ”€â”€ smdp_q_values.png
â”‚       â””â”€â”€ intraop_q_values.png
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ options/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Setup & Installation

```bash
git clone https://github.com/ahmecse/CS6700-PA3-HRL-Taxi.git
cd CS6700-PA3-HRL-Taxi
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

Run all experiments in Jupyter Lab:

```bash
cd notebooks
jupyter lab
```

Open `hrl_taxi_domain_analysis.ipynb`, set hyperparameters, and execute cells.

---

## ğŸ“Š Results

### Reward Curves: Individual & Comparison


| SMDP: Average reward per episode | IntraOP: Average reward per episode | Comparison: SMDP vs. Intra-Option |
|:--------------------------------:|:----------------------------------:|:---------------------------------:|
| ![](https://github.com/ahmecse/RL-Assignments-3-IITM-CS6700/raw/main/results/smdp/rewards.jpg) | ![](https://github.com/ahmecse/RL-Assignments-3-IITM-CS6700/raw/main/results/intraop/rewards.jpg) | ![](https://github.com/ahmecse/RL-Assignments-3-IITM-CS6700/raw/main/results/comparison_plots/comparison_plot.png) |



---

### Update Frequency Analysis
![](results/comparison_plots/Update%20Frequency_SMDP%20and%20Intra-Option%20Q-Learning.PNG)  
*How often each algorithmâ€™s Bellman updates fire over training*

---

### Learned Q-Values
<div style="display: flex; gap: 1em;">
  <img src="https://github.com/ahmecse/RL-Assignments-3-IITM-CS6700/raw/main/results/smdp/a500_e100_g90_updates.jpg" alt="SMDP update frequency heatmap" style="width:45%;" />
  <img src="https://github.com/ahmecse/RL-Assignments-3-IITM-CS6700/raw/main/results/intraop/a500_e1_g90_updates.jpg" alt="Intra-Option update frequency heatmap" style="width:45%;" />
</div>

---

## ğŸ”– References

1. Sutton, R.Â S., Precup, D., & Singh, S.Â (1999). Between MDPs and semiâ€‘MDPs: A framework for temporal abstraction in reinforcement learning. *Artificial Intelligence*, 112(1â€‘2), 181â€“211.
2. Gymnasium Taxiâ€‘v3 Environment. Retrieved from [https://gymnasium.farama.org/environments/toy\_text/taxi/](https://gymnasium.farama.org/environments/toy_text/taxi/)

---

